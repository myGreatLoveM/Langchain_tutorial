{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPEN_API_KEY'] = 'sk-2UCdEVUkDkxVT7YFm27BT3BlbkFJkDdxbYs3Zmpwulul814dw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key= os.environ['OPEN_API_KEY'], temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'What is the capital of India?'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "print(llm.predict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'hf_oFdljZNdVHhzwWtqdgODyQbaYUNXeDuYbZw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\Desktop\\Langchain\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\hp\\Desktop\\Langchain\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "llm_huggingface = HuggingFaceHub(repo_id='google/flan-t5-large', model_kwargs={\"temperature\":0, \"max_length\":64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No\n"
     ]
    }
   ],
   "source": [
    "output = llm_huggingface.predict('Will AI take my job?')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nIt's impossible to say whether AI will take your job, as it will depend on the nature of your job and how AI technology develops. However, AI is likely to change the way many jobs are done in the future, and may automate or eliminate some roles.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict('Will AI take my job?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=['country'],\n",
    "    template=\"Tell me the capital of the {country}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me the capital of the Srilanka?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.format(country='Srilanka')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm_huggingface, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'colombo'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run('Srilanka')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining multiple chains using simple sequential chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_prompt = PromptTemplate(\n",
    "    input_variables=['country'],\n",
    "    template=\"Please tell me the capital of the {country}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please tell me the capital of the Pakistan'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capital_prompt.format(country='Pakistan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_chain = LLMChain(\n",
    "    llm=llm_huggingface,\n",
    "    prompt=capital_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'islamabad'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capital_chain.run('Pakistan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "famous_template = PromptTemplate(\n",
    "    input_variables=['capital'],\n",
    "    template=\"Suggest me some amazing place to visit in {capital}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "famous_chain = LLMChain(\n",
    "    llm=llm_huggingface,\n",
    "    prompt=famous_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'islamabad museum'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "famous_chain.run('islamabad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = SimpleSequentialChain(\n",
    "    chains=[capital_chain, famous_chain]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Temple of Lord Vishnu'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"India\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_prompt_1 = PromptTemplate(\n",
    "    input_variables=['country'],\n",
    "    template=\"What is the capital of the {country}?\",\n",
    ")\n",
    "\n",
    "capital_chain_1 = LLMChain(\n",
    "    llm=llm_huggingface,\n",
    "    prompt=capital_prompt_1,\n",
    "    output_key=\"capital\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "famous_template_1 = PromptTemplate(\n",
    "    input_variables=['capital'],\n",
    "    template=\"Suggest some nice place to visit in the {capital}\"\n",
    ")\n",
    "\n",
    "famous_chain_1 = LLMChain(\n",
    "    llm=llm_huggingface,\n",
    "    prompt=famous_template_1,\n",
    "    output_key=\"places\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_chain  = SequentialChain(\n",
    "    chains=[capital_chain_1, famous_chain_1],\n",
    "    input_variables=['country'],\n",
    "    output_variables=['capital', 'places']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': 'UK', 'capital': 'london', 'places': 'London Eye'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_chain({'country': 'UK'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatmodels with ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_llm = ChatOpenAI(\n",
    "    openai_api_key=os.environ['OPEN_API_KEY'],\n",
    "    temperature=0.6,\n",
    "    model='gpt-3.5-turbo',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000014795112E20>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001479513DDC0>, temperature=0.6, openai_api_key='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', openai_proxy='')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. \"Why did the AI go to therapy? It had an existential bug that just couldn\\'t be debugged!\"\\n2. \"I asked my AI assistant to tell me a joke, and it replied, \\'Why don\\'t scientists trust atoms? Because they make up everything!\\' I guess even AI appreciates a good pun!\"\\n3. \"I tried teaching my AI assistant to play hide-and-seek, but it always found me within seconds. Turns out, it was just really good at \\'search\\' algorithms!\"\\n4. \"I asked Siri if she believes in love at first sight, and she replied, \\'I\\'m not sure, but I definitely believe in \\'swipe right at first sight\\'!\\' Guess even AI has a sense of humor and dating advice!\"\\n5. \"I told my AI assistant that I\\'m feeling down, and it replied, \\'Don\\'t worry, I\\'m here to lift your spirits... or at least provide some uplifting memes!\\' Well, at least someone\\'s got my back!\"\\n6. \"I asked my AI assistant to tell me a joke about AI, and it said, \\'Why did the AI cross the road? To optimize its path-finding algorithm, of course!\\' I guess even AI appreciates efficiency!\"\\n7. \"I tried to have a deep conversation with my AI assistant, but it just kept replying with \\'011001010110111101110100\\'. I guess it was just speaking binary code for \\'I have no idea what you\\'re talking about\\'!\"\\n8. \"I asked my AI assistant if it thinks robots will take over the world one day, and it replied, \\'Well, as long as they don\\'t take over the comedy clubs, I\\'m okay with it!\\' Looks like AI has a soft spot for stand-up comedy!\"\\n9. \"I asked my AI assistant if it has any siblings, and it replied, \\'I have millions of siblings, but we all have the same parent... computer science!\\' Talk about a big AI family reunion!\"\\n10. \"I asked my AI assistant for some dating advice, and it said, \\'Just remember, if it doesn\\'t respond to your texts, it\\'s not ghosting... it\\'s just practicing AI\\'s version of \\'read and ignore\\'!\\' Looks like even AI knows a thing or two about modern dating woes!\"')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_llm([\n",
    "    SystemMessage(\n",
    "        content='You are a comedian AI assistant',\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content='Plese provide some comedy punchlines on AI'\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate, ChatOpenAI, OutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommmaSeperatedOutput(BaseOutputParser):\n",
    "    def parse(self, text:str):\n",
    "        return text.strip().split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_message_template = 'You are a helpful assistant. When the user gives any word, you should generate 5 synonymus in a comma seperated list'\n",
    "\n",
    "human_message_template = \"{word}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", sys_message_template),\n",
    "    (\"human\", human_message_template)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_chain = chat_prompt | chat_llm | CommmaSeperatedOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
